{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2OAPwCaldRJ"
      },
      "outputs": [],
      "source": [
        "import spotipy\n",
        "import os\n",
        "import pandas as pd\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "\n",
        "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=\"a408aff0d2144935b690af228cd17f43\",\n",
        "                                                           client_secret=\"2f085d237f604ca6a04b63947290e55c\"))\n",
        "\n",
        "artist_ids = ['137W8MRPWKqSmrBGDBFSop', ] #'7dGJo4pcD2V6oG8kP0tJRR', '7vk5e3vY1uw9plTHJAMwjN', '53XhwfbYqKCa1cC15pYq2q', '1Cs0zKBU1kc0i8ypK3B9ai', '15UsOTVnJzReFVN1VCnxy4', '1Xyo4u8uXC1ZmMpatF05PJ', '06HL4z0CvFAxyc27GXpf02', '246dkjvS1zLTtiykXe5h60', '64KEffDW9EtZ1y2vBYgq8T', '66CXWjxzNUsdJxJ2JdwvnR', '1uNFoZAHBGtllmzznpCI3s', '60d24wfXkVzDSfLS6hyCjZ', '6eUKZXaKkcviH0Ku9w2n3V', '6M2wZ9GZgrQXHCFfjv46we', '4VMYDCV2IEDYJArk749S6m', '69GGBxA162lTqCwzJG5jLp', '0du5cEVh5yTK9QJze8zA0C', '41MozSoPIsD1dJM0CLPjZF', '6qqNVTkY8uBg9cP3Jd7DAH', '4q3ewBCX7sLwd24euuV69X', '3TVXtAsR1Inumwj472S9r4', '4AK6F7OLvEQ5QYCBNiQWHq', '6jJ0s89eD6GaHleKKya26X', '5pKCCKE2ajJHZ9KAiaK11H', '0TnOYISbd1XYRBk9myaseg', '0Y5tJX1MQlPlqiwlOH1tJY', '04gDigrS5kc9YWfZHwBETP'\n",
        "\n",
        "\n",
        "for artist_id in artist_ids:\n",
        "    try:\n",
        "        top_tracks = sp.artist_top_tracks(artist_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching top tracks for artist {artist_id}: {e}\")\n",
        "        continue  # Skip to the next artist if there's an error\n",
        "\n",
        "    albums = []\n",
        "    try:\n",
        "        results = sp.artist_albums(artist_id, album_type='album', limit=30)\n",
        "        if results and 'items' in results:\n",
        "            albums.extend(results['items'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching albums for artist {artist_id}: {e}\")\n",
        "        continue  # Skip to the next artist if there's an error\n",
        "\n",
        "    while results and results.get('next') and len(albums) < 50:\n",
        "        try:\n",
        "            results = sp.next(results)\n",
        "            if results and 'items' in results:\n",
        "                albums.extend(results['items'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching additional albums for artist {artist_id}: {e}\")\n",
        "            break  # Exit the while loop if there's an error\n",
        "\n",
        "    tracks = []\n",
        "    for album in albums:\n",
        "        album_tracks = sp.album_tracks(album['id'])['items']\n",
        "        for track in album_tracks:\n",
        "            if len(tracks) < 50:\n",
        "                track_details = sp.track(track['id'])\n",
        "                tracks.append(track_details)\n",
        "            else:\n",
        "                break\n",
        "        if len(tracks) >= 50:\n",
        "            break\n",
        "    tracks_data = []\n",
        "    for idx, track in enumerate(tracks):\n",
        "\n",
        "\n",
        "        audio_features = sp.audio_features(track['id'])[0]\n",
        "\n",
        "        track_info = {\n",
        "            \"Track Name\": track['name'],\n",
        "            \"Artist(s)\": \", \".join([artist['name'] for artist in track['artists']]),\n",
        "            \"Album\": track['album']['name'],\n",
        "            \"Release Date\": track['album']['release_date'],\n",
        "            \"Duration (ms)\": track['duration_ms'],\n",
        "            \"Popularity\": track['popularity'],\n",
        "            \"Available Markets:\": \", \".join(track['available_markets']),\n",
        "            \"Preview URL:\": track['preview_url'],\n",
        "            \"External URLs:\": track['external_urls']['spotify'],\n",
        "\n",
        "        }\n",
        "\n",
        "        # Append audio features if available\n",
        "        if audio_features:\n",
        "            track_info.update({\n",
        "                \"Danceability\": audio_features['danceability'],\n",
        "                \"Energy\": audio_features['energy'],\n",
        "                \"Key\": audio_features['key'],\n",
        "                \"Loudness\": audio_features['loudness'],\n",
        "                \"Mode\": audio_features['mode'],\n",
        "                \"Speechiness\": audio_features['speechiness'],\n",
        "                \"Acousticness\": audio_features['acousticness'],\n",
        "                \"Instrumentalness\": audio_features['instrumentalness'],\n",
        "                \"Liveness\": audio_features['liveness'],\n",
        "                \"Valence\": audio_features['valence'],\n",
        "                \"Tempo\": audio_features['tempo'],\n",
        "                \"Time Signature\": audio_features['time_signature']\n",
        "            })\n",
        "        tracks_data.append(track_info)\n",
        "\n",
        "    df = pd.DataFrame(tracks_data)\n",
        "\n",
        "    csv_file_path = 'spotify_data_ver1.csv'  # Replace with your CSV file path\n",
        "\n",
        "    # Check if the CSV file exists\n",
        "    if os.path.exists(csv_file_path):\n",
        "        # Append without writing the header\n",
        "        df.to_csv(csv_file_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        # Write new file with header\n",
        "        df.to_csv(csv_file_path, mode='w', index=False, header=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"spotify_data_ver1.csv\")\n",
        "print(df.columns)\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "Efk-dctulxNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "MfBXAw8-l64Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Available Markets:'].fillna('No Available Markets provided', inplace=True)\n",
        "df['Preview URL:'].fillna('No Preview URL provided', inplace=True)\n",
        "df['Preview URL:'].fillna('No Preview URL provided', inplace=True)\n",
        "columns_to_replace = [\n",
        "    \"Danceability\", \"Energy\", \"Key\", \"Loudness\", \"Mode\",\n",
        "    \"Speechiness\", \"Acousticness\", \"Instrumentalness\",\n",
        "    \"Liveness\", \"Valence\", \"Tempo\", \"Time Signature\"\n",
        "]\n",
        "\n",
        "df[columns_to_replace] = df[columns_to_replace].fillna(0.0)"
      ],
      "metadata": {
        "id": "UlAnhUOpl9y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "ECD0scvvmAPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'Available Markets:': 'Available_Markets'}, inplace=True)\n",
        "df.rename(columns={'Preview URL:': 'Preview_URL'}, inplace=True)\n",
        "df.rename(columns={'External URLs:': 'External_URLs'}, inplace=True)\n",
        "df.rename(columns={'Artist(s)': 'Artist'}, inplace=True)\n",
        "df.rename(columns={'Duration (ms)': 'Duration'}, inplace=True)\n",
        "df.rename(columns={'Track Name': 'Track_Name'}, inplace=True)\n",
        "df.rename(columns={'Release Date': 'Release_Date'}, inplace=True)\n",
        "\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "KSe7pv_RmEVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_date(date_str):\n",
        "    try:\n",
        "        date = pd.to_datetime(date_str, format='%Y-%m-%d')\n",
        "    except ValueError:\n",
        "        # Assuming that if it's not a full date, it's just a year\n",
        "        date = pd.to_datetime(date_str, format='%Y')\n",
        "    return date.strftime('%d-%m-%Y')\n",
        "\n",
        "# Apply the function to the Release_Date column\n",
        "df['Release_Date'] = df['Release_Date'].apply(standardize_date)\n",
        "\n",
        "df['Release_Date'] = pd.to_datetime(df['Release_Date'], format='%d-%m-%Y')\n"
      ],
      "metadata": {
        "id": "Ug1V0ROamHW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Duration'] = df['Duration'] / 1000\n",
        "# Convert 'Duration' from seconds to timedelta\n",
        "df['Duration'] = pd.to_timedelta(df['Duration'], unit='s')\n"
      ],
      "metadata": {
        "id": "ePHGIViAmK5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "def fetch_videos(api_key, channel_id, max_results=100):\n",
        "    base_url = \"https://www.googleapis.com/youtube/v3/\"\n",
        "\n",
        "    # Define 'part' and 'fields' parameters for videos\n",
        "    part_param = \"snippet\"\n",
        "    fields_param = f\"items(id,snippet/title,snippet/publishedAt,snippet/channelId)&maxResults={max_results}\"\n",
        "\n",
        "    # Make the API request for videos\n",
        "    url = f\"{base_url}search?key={api_key}&part={part_param}&channelId={channel_id}&{fields_param}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Process the response\n",
        "    if response.status_code == 200:\n",
        "        search_results = response.json()\n",
        "        video_ids = [item[\"id\"][\"videoId\"] if \"videoId\" in item[\"id\"] else item[\"id\"].get(\"channelId\", \"\") for item in search_results.get(\"items\", [])]\n",
        "\n",
        "        # Fetch channel details\n",
        "        channel_data = fetch_channel_details(api_key, channel_id)\n",
        "        channel_name = channel_data.get(\"items\", [])[0][\"snippet\"][\"title\"] if channel_data else \"N/A\"\n",
        "\n",
        "        videos_data = fetch_video_details(api_key, video_ids)\n",
        "        return videos_data, channel_name\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        return None, None\n",
        "\n",
        "def fetch_video_details(api_key, video_ids):\n",
        "    base_url = \"https://www.googleapis.com/youtube/v3/\"\n",
        "\n",
        "    # Define 'part' parameter for video details\n",
        "    part_param = \"snippet,contentDetails,statistics\"\n",
        "\n",
        "    # Convert video_ids list to a comma-separated string\n",
        "    video_ids_str = \",\".join(video_ids)\n",
        "\n",
        "    # Make the API request for video details\n",
        "    url = f\"{base_url}videos?key={api_key}&part={part_param}&id={video_ids_str}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Process the response\n",
        "    if response.status_code == 200:\n",
        "        videos_data = response.json()\n",
        "        return videos_data\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        return None\n",
        "\n",
        "def fetch_channel_details(api_key, channel_id):\n",
        "    base_url = \"https://www.googleapis.com/youtube/v3/\"\n",
        "\n",
        "    # Define 'part' parameter for channel details\n",
        "    part_param = \"snippet\"\n",
        "\n",
        "    # Make the API request for channel details\n",
        "    url = f\"{base_url}channels?key={api_key}&part={part_param}&id={channel_id}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Process the response\n",
        "    if response.status_code == 200:\n",
        "        channel_data = response.json()\n",
        "        return channel_data\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "  # Change key and id\n",
        "    api_key = \"AIzaSyBHmvcOcL_2aQO0PKxEBgG8QJSPfKVsMPU\"\n",
        "    channel_id = \"UCfM3zsQsOnfWNUppiycmBuw\"\n",
        "    max_results = 1\n",
        "\n",
        "    videos_data, channel_name = fetch_videos(api_key, channel_id, max_results)\n",
        "\n",
        "    if videos_data:\n",
        "        # Process the retrieved videos data\n",
        "        # Process the retrieved videos data\n",
        "          for item in videos_data.get(\"items\", []):\n",
        "            video_title = item[\"snippet\"][\"title\"]\n",
        "            published_at = item[\"snippet\"][\"publishedAt\"]\n",
        "\n",
        "    # Additional details\n",
        "            views = item[\"statistics\"][\"viewCount\"] if \"statistics\" in item else \"N/A\"\n",
        "            comments = item[\"statistics\"][\"commentCount\"] if \"statistics\" in item else \"N/A\"\n",
        "            likes = item[\"statistics\"][\"likeCount\"] if \"statistics\" in item else \"N/A\"\n",
        "            if \"dislikeCount\" in item.get(\"statistics\", {}):\n",
        "                dislikes = item[\"statistics\"][\"dislikeCount\"]\n",
        "            else:import requests\n",
        "import csv\n",
        "\n",
        "def fetch_videos(api_key, channel_id, max_results=100):\n",
        "    base_url = \"https://www.googleapis.com/youtube/v3/\"\n",
        "\n",
        "    # Define 'part' and 'fields' parameters for videos\n",
        "    part_param = \"snippet\"\n",
        "    fields_param = f\"items(id,snippet/title,snippet/publishedAt,snippet/channelId)&maxResults={max_results}\"\n",
        "\n",
        "    # Make the API request for videos\n",
        "    url = f\"{base_url}search?key={api_key}&part={part_param}&channelId={channel_id}&{fields_param}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Process the response\n",
        "    if response.status_code == 200:\n",
        "        search_results = response.json()\n",
        "        video_ids = [item[\"id\"][\"videoId\"] if \"videoId\" in item[\"id\"] else item[\"id\"].get(\"channelId\", \"\") for item in search_results.get(\"items\", [])]\n",
        "\n",
        "        # Fetch channel details\n",
        "        channel_data = fetch_channel_details(api_key, channel_id)\n",
        "        channel_name = channel_data.get(\"items\", [])[0][\"snippet\"][\"title\"] if channel_data else \"N/A\"\n",
        "\n",
        "        videos_data = fetch_video_details(api_key, video_ids)\n",
        "        return videos_data, channel_name\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        return None, None\n",
        "\n",
        "def fetch_video_details(api_key, video_ids):\n",
        "    base_url = \"https://www.googleapis.com/youtube/v3/\"\n",
        "\n",
        "    # Define 'part' parameter for video details\n",
        "    part_param = \"snippet,contentDetails,statistics\"\n",
        "\n",
        "    # Convert video_ids list to a comma-separated string\n",
        "    video_ids_str = \",\".join(video_ids)\n",
        "\n",
        "    # Make the API request for video details\n",
        "    url = f\"{base_url}videos?key={api_key}&part={part_param}&id={video_ids_str}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Process the response\n",
        "    if response.status_code == 200:\n",
        "        videos_data = response.json()\n",
        "        return videos_data\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        return None\n",
        "\n",
        "def fetch_channel_details(api_key, channel_id):\n",
        "    base_url = \"https://www.googleapis.com/youtube/v3/\"\n",
        "\n",
        "    # Define 'part' parameter for channel details\n",
        "    part_param = \"snippet\"\n",
        "\n",
        "    # Make the API request for channel details\n",
        "    url = f\"{base_url}channels?key={api_key}&part={part_param}&id={channel_id}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Process the response\n",
        "    if response.status_code == 200:\n",
        "        channel_data = response.json()\n",
        "        return channel_data\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        return None\n",
        "\n",
        "# Function to write video details to a CSV file\n",
        "def write_to_csv(file_path, video_data):\n",
        "    fieldnames = [\n",
        "        \"Title\", \"Published At\", \"Channel Title\",\n",
        "        \"Views\", \"Likes\",  \"Comments\",\n",
        "        \"Description\", \"Tags\", \"Duration\",\n",
        "    ]\n",
        "\n",
        "    with open(file_path, mode=\"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "        # Write header\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write video details\n",
        "        for item in video_data.get(\"items\", []):\n",
        "            video_title = item[\"snippet\"][\"title\"]\n",
        "            published_at = item[\"snippet\"][\"publishedAt\"]\n",
        "            views = item[\"statistics\"][\"viewCount\"] if \"statistics\" in item else \"N/A\"\n",
        "            #comments = item[\"statistics\"][\"commentCount\"] if \"statistics\" in item else \"N/A\"\n",
        "            comments = item[\"statistics\"].get(\"commentCount\", 0)\n",
        "            likes = item[\"statistics\"][\"likeCount\"] if \"statistics\" in item else \"N/A\"\n",
        "\n",
        "            channel_title = item[\"snippet\"][\"channelTitle\"]\n",
        "            description = item[\"snippet\"][\"description\"]\n",
        "            tags = item[\"snippet\"][\"tags\"] if \"tags\" in item[\"snippet\"] else []\n",
        "            duration = item[\"contentDetails\"][\"duration\"] if \"contentDetails\" in item else \"N/A\"\n",
        "\n",
        "            # Write row\n",
        "            writer.writerow({\n",
        "                \"Title\": video_title,\n",
        "                \"Published At\": published_at,\n",
        "                \"Channel Title\": channel_title,\n",
        "                \"Views\": views,\n",
        "                \"Likes\": likes,\n",
        "                \"Comments\": comments,\n",
        "                \"Description\": description,\n",
        "                \"Tags\": ', '.join(tags),\n",
        "                \"Duration\": duration,\n",
        "            })\n",
        "\n",
        "def main():\n",
        "    # Change key and id\n",
        "    api_key = \"AIzaSyBHmvcOcL_2aQO0PKxEBgG8QJSPfKVsMPU\"\n",
        "    channel_id = \"UC5H_KXkPbEsGs0tFt8R35mA\"\n",
        "    max_results = 200\n",
        "\n",
        "    videos_data, channel_name = fetch_videos(api_key, channel_id, max_results)\n",
        "\n",
        "    if videos_data:\n",
        "        # Process the retrieved videos data\n",
        "        for item in videos_data.get(\"items\", []):\n",
        "            video_title = item[\"snippet\"][\"title\"]\n",
        "            published_at = item[\"snippet\"][\"publishedAt\"]\n",
        "\n",
        "            # Additional details\n",
        "            views = item[\"statistics\"][\"viewCount\"] if \"statistics\" in item else \"N/A\"\n",
        "            #comments = item[\"statistics\"][\"commentCount\"] if \"statistics\" in item else \"N/A\"\n",
        "            comments = item[\"statistics\"].get(\"commentCount\", 0)\n",
        "            likes = item[\"statistics\"][\"likeCount\"] if \"statistics\" in item else \"N/A\"\n",
        "\n",
        "            # Additional details\n",
        "            channel_title = item[\"snippet\"][\"channelTitle\"]\n",
        "            description = item[\"snippet\"][\"description\"]\n",
        "            tags = item[\"snippet\"][\"tags\"] if \"tags\" in item[\"snippet\"] else []\n",
        "\n",
        "            # Content details\n",
        "            duration = item[\"contentDetails\"][\"duration\"] if \"contentDetails\" in item else \"N/A\"\n",
        "\n",
        "\n",
        "            # Create a list for each video\n",
        "            video_details = [\n",
        "                f\"Title: {video_title}\",\n",
        "                f\"Published At: {published_at}\",\n",
        "                f\"Channel Title: {channel_title}\",\n",
        "                f\"Views: {views}\",\n",
        "                f\"Likes: {likes}\",\n",
        "                f\"Comments: {comments}\",\n",
        "                f\"Description: {description}\",\n",
        "                f\"Tags: {', '.join(tags)}\",\n",
        "                f\"Duration: {duration}\"\n",
        "            ]\n",
        "\n",
        "            # Print the list for each video\n",
        "            print(video_details)\n",
        "\n",
        "        # Specify the path for the CSV file\n",
        "        csv_file_path = \"MartinGarrix_video_details.csv\"\n",
        "\n",
        "        # Write video details to CSV\n",
        "        write_to_csv(csv_file_path, videos_data)\n",
        "        print(f\"\\nVideo details have been written to {csv_file_path}\")\n",
        "        # Download the CSV file\n",
        "        from google.colab import files\n",
        "        files.download(csv_file_path)\n",
        "    else:\n",
        "        print(\"Failed to fetch videos data.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "                dislikes = \"N/A\"\n",
        "    # Additional details\n",
        "            channel_title = item[\"snippet\"][\"channelTitle\"]\n",
        "            description = item[\"snippet\"][\"description\"]\n",
        "            tags = item[\"snippet\"][\"tags\"] if \"tags\" in item[\"snippet\"] else []\n",
        "\n",
        "            duration = item[\"contentDetails\"][\"duration\"] if \"contentDetails\" in item else \"N/A\"\n",
        "\n",
        "            privacy_status = item[\"status\"][\"privacyStatus\"] if \"status\" in item else \"N/A\"\n",
        "\n",
        "            video_details = [\n",
        "            f\"Title: {video_title}\",\n",
        "            f\"Published At: {published_at}\",\n",
        "            f\"Channel Title: {channel_title}\",\n",
        "            f\"Views: {views}\",\n",
        "            f\"Likes: {likes}\",\n",
        "            f\"Dislikes: {dislikes}\",\n",
        "            f\"Comments: {comments}\",\n",
        "            f\"Description: {description}\",\n",
        "            f\"Tags: {', '.join(tags)}\",\n",
        "            f\"Duration: {duration}\",\n",
        "            f\"Privacy Status: {privacy_status}\\n\"\n",
        "            ]\n",
        "\n",
        "            print(video_details)\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to fetch videos data.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mx13KuRlmM-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import isodate"
      ],
      "metadata": {
        "id": "SXdzobZzmW-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"videos_data_ver1.csv\")\n",
        "print(df['Channel Title'].value_counts())"
      ],
      "metadata": {
        "id": "5M6aDnnbmgFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "df['Description'].fillna('No description provided', inplace=True)\n",
        "df.drop('Privacy Status', axis=1, inplace=True)\n",
        "\n",
        "df['Views'] = pd.to_numeric(df['Views'], errors='coerce').fillna(0).astype(int)\n",
        "df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce').fillna(0).astype(int)\n",
        "df['Comments'] = pd.to_numeric(df['Comments'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "updated_missing_values = df.isnull().sum()\n",
        "updated_data_types = df.dtypes\n",
        "\n",
        "updated_missing_values, updated_data_types\n",
        "\n"
      ],
      "metadata": {
        "id": "VBJQC1jJmi4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/your/service-account-file.json\"\n",
        "client = bigquery.Client()\n",
        "dataset_id = 'artist_details'\n",
        "table_id = 'spotify_data'\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,  # Change this accordingly if your CSV has headers\n",
        "    autodetect=True,      # Automatically detect schema\n",
        ")\n",
        "with open(\"spotify_data_ver4.csv\", \"rb\") as file:\n",
        "    load_job = client.load_table_from_file(\n",
        "        file,\n",
        "        f\"{dataset_id}.{table_id}\",\n",
        "        job_config=job_config\n",
        "    )  # Make an API request.\n",
        "load_job.result()  # Waits for the job to complete.\n",
        "if load_job.errors:\n",
        "    print(\"Errors:\", load_job.errors)\n",
        "else:\n",
        "    print(\"Upload completed successfully.\")"
      ],
      "metadata": {
        "id": "4fGqf5Ze2Tr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}